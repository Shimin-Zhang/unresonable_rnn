import type { QuizConfig } from '@/components/interactive/quiz/types'

export const MODULE_9_QUIZ: QuizConfig = {
  id: 'module-9-implementation',
  title: 'Implementation Deep Dive - Knowledge Check',
  description: 'Test your understanding of RNN implementation, training techniques, and troubleshooting.',
  questions: [
    {
      id: 'q1-track-selection',
      type: 'multiple_choice',
      question: 'Which implementation track is BEST for quickly prototyping a text generation system with minimal code?',
      options: [
        'Track A (NumPy) - for deep understanding',
        'Track B (PyTorch) - for custom solutions',
        'Track C (Hugging Face) - for pretrained models',
        'All tracks are equally fast',
      ],
      correctAnswer: 2,
      explanation: 'Hugging Face (Track C) leverages pretrained models like GPT-2 that already encode linguistic knowledge. Fine-tuning takes 1-2 hours and requires minimal code. NumPy is for learning, PyTorch is for custom solutions requiring more control.',
      points: 10,
    },
    {
      id: 'q2-gradient-clipping',
      type: 'multiple_choice',
      question: 'Why is gradient clipping essential for RNN training?',
      options: [
        'It makes training faster by removing unnecessary gradients',
        'It prevents exploding gradients that cause NaN losses',
        'It ensures all gradients are positive',
        'It reduces memory usage',
      ],
      correctAnswer: 1,
      explanation: 'In BPTT, gradients are multiplied across timesteps. When these products grow large (exploding gradients), loss becomes NaN. Gradient clipping caps the gradient norm (e.g., to 1.0), preventing this explosion while preserving gradient direction.',
      points: 10,
    },
    {
      id: 'q3-bptt',
      type: 'multiple_choice',
      question: 'What does "Truncated BPTT" trade off for computational efficiency?',
      options: [
        'Model accuracy for speed',
        'Some gradient accuracy for memory and compute savings',
        'Training stability for faster convergence',
        'Vocabulary size for sequence length',
      ],
      correctAnswer: 1,
      explanation: 'Truncated BPTT only propagates gradients through chunks of the sequence (e.g., 25-100 steps) rather than the entire sequence. This saves memory and compute, but gradients from early timesteps to late timesteps are not captured. In practice, this tradeoff works well.',
      points: 10,
    },
    {
      id: 'q4-failure-modes',
      type: 'matching',
      question: 'Match each training symptom to its most likely solution:',
      pairs: [
        { id: 'p1', left: 'Loss is NaN', right: 'Add gradient clipping' },
        { id: 'p2', left: 'Output repeats same phrase', right: 'Increase temperature or add dropout' },
        { id: 'p3', left: 'Loss stuck high', right: 'Use LSTM instead of vanilla RNN' },
        { id: 'p4', left: 'Out of memory', right: 'Use gradient accumulation or smaller batch' },
      ],
      explanation: 'NaN loss typically means exploding gradients - clip them. Repetitive output indicates mode collapse or low temperature. High stuck loss often means vanishing gradients - use LSTM. OOM requires reducing memory footprint via smaller batches or gradient accumulation.',
      points: 20,
    },
    {
      id: 'q5-adam-optimizer',
      type: 'multiple_choice',
      question: 'Why is Adam optimizer often preferred over SGD for RNN training?',
      options: [
        'Adam uses less memory',
        'Adam adapts learning rates per parameter, handling varying gradient scales',
        'Adam guarantees convergence',
        'Adam only works with RNNs',
      ],
      correctAnswer: 1,
      explanation: 'RNN gradients vary dramatically across parameters and timesteps. Adam maintains per-parameter adaptive learning rates based on gradient history (first and second moments). This handles the varying scales better than fixed learning rate SGD.',
      points: 10,
    },
    {
      id: 'q6-dropout-placement',
      type: 'multiple_choice',
      question: 'Where should dropout be applied in an LSTM?',
      options: [
        'Only on the input layer',
        'On non-recurrent connections (between layers, before output)',
        'On the recurrent connections within each timestep',
        'Dropout should never be used with LSTMs',
      ],
      correctAnswer: 1,
      explanation: 'Standard dropout is applied to non-recurrent connections: between LSTM layers and before the output layer. Applying dropout to recurrent connections can harm the ability to maintain long-term dependencies. Variational dropout exists for recurrent connections but is more complex.',
      points: 10,
    },
    {
      id: 'q7-temperature',
      type: 'multiple_choice',
      question: 'What happens when you increase the temperature parameter during text generation?',
      options: [
        'Output becomes more deterministic and repetitive',
        'Output becomes more random and diverse',
        'Generation speed increases',
        'Model uses more memory',
      ],
      correctAnswer: 1,
      explanation: 'Temperature scales the logits before softmax. Higher temperature (>1) flattens the probability distribution, making unlikely tokens more probable and output more diverse/random. Lower temperature (<1) sharpens the distribution, making output more deterministic.',
      points: 10,
    },
    {
      id: 'q8-lr-schedule',
      type: 'multiple_choice',
      question: 'Why use learning rate scheduling (e.g., ReduceLROnPlateau)?',
      options: [
        'To make training faster from the start',
        'High LR finds good regions quickly; lower LR fine-tunes without overshooting',
        'To reduce memory usage during training',
        'Learning rate scheduling is only for Transformers',
      ],
      correctAnswer: 1,
      explanation: 'Starting with a higher learning rate helps find good loss basins quickly. As training progresses and loss plateaus, reducing the LR allows fine-grained optimization without overshooting minima. ReduceLROnPlateau automatically reduces LR when validation loss stops improving.',
      points: 10,
    },
  ],
  shuffleQuestions: false,
  shuffleOptions: false,
  showFeedback: 'immediate',
  allowRetry: true,
  passingScore: 70,
}
