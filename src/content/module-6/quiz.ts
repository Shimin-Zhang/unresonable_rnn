import type { QuizConfig } from '@/components/interactive/quiz/types'

export const MODULE_6_QUIZ: QuizConfig = {
  id: 'module-6-beyond-text',
  title: 'Beyond Text - Knowledge Check',
  description: 'Test your understanding of encoder-decoder architectures and multimodal RNN applications.',
  questions: [
    {
      id: 'q1-encoder-decoder-purpose',
      type: 'multiple_choice',
      question: 'What is the main purpose of the encoder-decoder architecture?',
      options: [
        'To compress input into a context vector, then generate output from it',
        'To run two networks in parallel for faster training',
        'To separate training and inference phases',
        'To handle only fixed-length sequences',
      ],
      correctAnswer: 0,
      explanation: 'The encoder-decoder (seq2seq) architecture compresses variable-length input into a fixed-size context vector (encoding), then generates variable-length output from that context (decoding). This enables transforming sequences of different lengths, like translating a 5-word French sentence into a 7-word English sentence.',
      points: 10,
    },
    {
      id: 'q2-context-vector-bottleneck',
      type: 'multiple_choice',
      question: 'What is the "bottleneck problem" in encoder-decoder architectures?',
      options: [
        'The encoder is slower than the decoder',
        'The fixed-size context vector must capture all information from the input',
        'The decoder can only generate short sequences',
        'The architecture requires too much memory',
      ],
      correctAnswer: 1,
      explanation: 'The bottleneck problem refers to compressing an entire input sequence (which could be very long) into a single fixed-size context vector. For long sequences, this vector cannot capture all the nuances, leading to degraded performance. This limitation motivated the development of attention mechanisms.',
      points: 10,
    },
    {
      id: 'q3-image-captioning-architecture',
      type: 'matching',
      question: 'Match each component in image captioning to its role:',
      pairs: [
        { id: 'p1', left: 'CNN (e.g., VGG, ResNet)', right: 'Extracts visual features from image' },
        { id: 'p2', left: 'RNN/LSTM', right: 'Generates words sequentially' },
        { id: 'p3', left: 'Softmax layer', right: 'Produces probability distribution over vocabulary' },
        { id: 'p4', left: 'Word embeddings', right: 'Converts words to dense vectors' },
      ],
      explanation: 'In image captioning, the CNN acts as a visual encoder extracting features (typically from the penultimate layer). These features initialize or feed into the RNN, which generates words one at a time. Each word is converted to an embedding and fed back as input. The softmax layer outputs probabilities for the next word.',
      points: 20,
    },
    {
      id: 'q4-translation-innovation',
      type: 'multiple_choice',
      question: 'Which innovation surprisingly improved neural machine translation significantly?',
      options: [
        'Making the model larger',
        'Reversing the source sentence before encoding',
        'Using only lowercase text',
        'Training on more languages simultaneously',
      ],
      correctAnswer: 1,
      explanation: 'Sutskever et al. found that reversing the source sentence ("Je suis etudiant" becomes "etudiant suis Je") improved translation quality significantly. This puts the first words of source and target closer together during backpropagation, making it easier for the model to learn alignments.',
      points: 10,
    },
    {
      id: 'q5-ctc-purpose',
      type: 'multiple_choice',
      question: 'In speech recognition, what problem does CTC (Connectionist Temporal Classification) solve?',
      options: [
        'Converting text to speech',
        'Handling variable alignment between audio frames and characters',
        'Compressing audio files',
        'Removing background noise',
      ],
      correctAnswer: 1,
      explanation: 'CTC handles the alignment problem: the same word spoken at different speeds produces different numbers of audio frames. CTC introduces a blank symbol and allows the model to output the same character multiple times, then collapses them. This enables training without manually aligning audio to text.',
      points: 10,
    },
    {
      id: 'q6-vqa-fusion',
      type: 'multiple_choice',
      question: 'How are visual and text features typically combined in Visual Question Answering (VQA)?',
      options: [
        'By training separate models and averaging predictions',
        'By concatenating or element-wise multiplying feature vectors',
        'By converting images to text first',
        'Visual features are not used in VQA',
      ],
      correctAnswer: 1,
      explanation: 'In VQA, the CNN extracts visual features and the LSTM encodes the question. These feature vectors are typically combined through concatenation, element-wise multiplication, or more sophisticated fusion methods (like attention). The combined representation is then passed through a classifier to predict the answer.',
      points: 10,
    },
    {
      id: 'q7-applications-matching',
      type: 'matching',
      question: 'Match each application to its input-output modalities:',
      pairs: [
        { id: 'p1', left: 'Image Captioning', right: 'Image to Text' },
        { id: 'p2', left: 'Machine Translation', right: 'Text to Text' },
        { id: 'p3', left: 'Speech Recognition', right: 'Audio to Text' },
        { id: 'p4', left: 'Text-to-Speech', right: 'Text to Audio' },
      ],
      explanation: 'RNNs became the "universal glue" for multimodal AI by enabling transformations between different modalities. Image captioning converts visual input to sequential text output. Machine translation transforms text between languages. Speech recognition converts audio to text. Text-to-speech does the reverse.',
      points: 20,
    },
    {
      id: 'q8-bidirectional-encoder',
      type: 'multiple_choice',
      question: 'Why do translation systems often use bidirectional encoders?',
      options: [
        'To double the training speed',
        'To give each position context from both past and future words',
        'To translate in both directions simultaneously',
        'To reduce memory usage',
      ],
      correctAnswer: 1,
      explanation: 'Bidirectional encoders run two RNNs - one forward and one backward - and concatenate their hidden states. This gives each position context from both directions: forward context captures what came before, backward context captures what comes after. This is crucial because meaning often depends on surrounding context in both directions.',
      points: 10,
    },
  ],
  shuffleQuestions: false,
  shuffleOptions: false,
  showFeedback: 'immediate',
  allowRetry: true,
  passingScore: 70,
}
