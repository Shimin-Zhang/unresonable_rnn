import type { QuizConfig } from '@/components/interactive/quiz/types'

export const MODULE_8_QUIZ: QuizConfig = {
  id: 'module-8-limitations',
  title: 'Limitations and Path Forward - Knowledge Check',
  description: 'Test your understanding of RNN limitations and when to use modern alternatives.',
  questions: [
    {
      id: 'q1-long-range',
      type: 'multiple_choice',
      question: 'Why do RNNs struggle with very long-range dependencies even with LSTM?',
      options: [
        'LSTMs are too slow to process long sequences',
        'Information must pass through every intermediate step, degrading along the way',
        'LSTMs can only handle sequences up to 100 tokens',
        'The forget gate always removes all past information',
      ],
      correctAnswer: 1,
      explanation: 'Even with LSTM gates, information from early positions must flow through all intermediate hidden states to reach later positions. This creates an O(n) path length where information can degrade. Transformers solve this with O(1) path length via direct attention.',
      points: 10,
    },
    {
      id: 'q2-sequential-bottleneck',
      type: 'multiple_choice',
      question: 'What is the "sequential processing bottleneck" in RNNs?',
      options: [
        'RNNs can only process one sequence at a time',
        'Each hidden state h_t depends on h_{t-1}, preventing parallelization',
        'RNNs require sequences to be sorted alphabetically',
        'Sequential data cannot be batched',
      ],
      correctAnswer: 1,
      explanation: 'In RNNs, computing h_t requires h_{t-1}, which requires h_{t-2}, and so on. This sequential dependency means you cannot compute all positions in parallel, limiting GPU utilization. Transformers compute all positions simultaneously via self-attention.',
      points: 10,
    },
    {
      id: 'q3-transformer-solutions',
      type: 'matching',
      question: 'Match each RNN limitation to how Transformers solve it:',
      pairs: [
        { id: 'p1', left: 'Long-range dependencies degrade', right: 'Direct attention to any position (O(1) path)' },
        { id: 'p2', left: 'Sequential processing bottleneck', right: 'Fully parallel attention computation' },
        { id: 'p3', left: 'Representation coupling', right: 'Separate Query/Key/Value projections' },
        { id: 'p4', left: 'Training instability', right: 'Layer norm + residual connections' },
      ],
      explanation: 'Transformers address each RNN limitation: self-attention provides direct paths between any positions, attention is computed in parallel across all positions, Q/K/V projections decouple representation from memory, and architectural innovations like layer norm and residuals stabilize training.',
      points: 20,
    },
    {
      id: 'q4-when-use-rnn',
      type: 'multiple_choice',
      question: 'Which scenario is BEST suited for using an RNN instead of a Transformer?',
      options: [
        'Training a chatbot on millions of conversations',
        'Processing real-time sensor data on an embedded device',
        'Building a document summarization system',
        'Fine-tuning a language model for code completion',
      ],
      correctAnswer: 1,
      explanation: 'RNNs excel at real-time streaming on memory-constrained devices. They have O(1) memory per step and can process infinite streams. Transformers need O(nÂ²) memory for attention and cannot easily handle streaming data. The other scenarios benefit from Transformer capabilities.',
      points: 10,
    },
    {
      id: 'q5-representation-coupling',
      type: 'multiple_choice',
      question: 'What does "representation coupling" mean as an RNN limitation?',
      options: [
        'RNNs can only learn one type of representation',
        'The hidden state must serve dual purposes: output representation AND memory',
        'RNN weights are coupled across layers',
        'Input and output representations must be the same size',
      ],
      correctAnswer: 1,
      explanation: 'The RNN hidden state must simultaneously encode what to output at the current step AND what information to preserve for future steps. These competing demands limit what the network can express. Transformers separate these concerns with distinct Q, K, V projections.',
      points: 10,
    },
    {
      id: 'q6-build-vs-buy',
      type: 'multiple_choice',
      question: 'For most standard NLP tasks in 2024+, what is the recommended approach?',
      options: [
        'Train an RNN from scratch for maximum control',
        'Train a Transformer from scratch for best performance',
        'Use a pretrained Transformer via API (GPT, Claude, etc.)',
        'Combine multiple RNNs in an ensemble',
      ],
      correctAnswer: 2,
      explanation: 'For most standard NLP tasks, using pretrained Transformer APIs is the fastest, most cost-effective approach. These models have been trained on massive data and work well out-of-the-box. Custom training only makes sense for specific requirements like proprietary data, extreme latency needs, or regulatory constraints.',
      points: 10,
    },
    {
      id: 'q7-transformer-timeline',
      type: 'multiple_choice',
      question: 'What was the key innovation in the 2017 "Attention Is All You Need" paper?',
      options: [
        'Adding attention to RNNs for better translation',
        'Replacing recurrence entirely with self-attention',
        'Making LSTMs deeper with more layers',
        'Introducing the forget gate mechanism',
      ],
      correctAnswer: 1,
      explanation: 'The Transformer paper\'s key insight was that recurrence was not necessary - self-attention alone could model sequences. By removing the sequential dependency, they enabled massive parallelization and better gradient flow, making it possible to train much larger models.',
      points: 10,
    },
    {
      id: 'q8-state-space',
      type: 'multiple_choice',
      question: 'What do modern state-space models like Mamba offer?',
      options: [
        'Even more attention heads than Transformers',
        'RNN-like efficiency with Transformer-like quality',
        'Faster training but worse inference',
        'Better performance only on image tasks',
      ],
      correctAnswer: 1,
      explanation: 'State-space models like Mamba combine the best of both worlds: they process sequences with RNN-like O(1) memory per step (good for streaming and long sequences) while achieving quality comparable to Transformers. They represent a potential new paradigm for sequence modeling.',
      points: 10,
    },
  ],
  shuffleQuestions: false,
  shuffleOptions: false,
  showFeedback: 'immediate',
  allowRetry: true,
  passingScore: 70,
}
