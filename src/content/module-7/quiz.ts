import type { QuizConfig } from '@/components/interactive/quiz/types'

export const MODULE_7_QUIZ: QuizConfig = {
  id: 'module-7-attention',
  title: 'Attention Mechanisms - Knowledge Check',
  description: 'Test your understanding of attention mechanisms and their role in modern AI.',
  questions: [
    {
      id: 'q1-bottleneck-problem',
      type: 'multiple_choice',
      question: 'What is the "bottleneck problem" in encoder-decoder architectures that attention solves?',
      options: [
        'The model trains too slowly',
        'All input information must be compressed into a single fixed-size vector',
        'The decoder cannot generate long outputs',
        'The encoder uses too much memory',
      ],
      correctAnswer: 1,
      explanation: 'The bottleneck problem refers to compressing the entire input sequence into a single fixed-size context vector. For long sequences, this vector cannot capture all necessary information. Attention solves this by allowing the decoder to access all encoder states directly.',
      points: 10,
    },
    {
      id: 'q2-attention-steps',
      type: 'matching',
      question: 'Match each step of the attention mechanism to its purpose:',
      pairs: [
        { id: 'p1', left: 'Score (e_{t,i})', right: 'Compute relevance of each encoder state' },
        { id: 'p2', left: 'Softmax (alpha)', right: 'Normalize scores to probabilities' },
        { id: 'p3', left: 'Weighted sum (c_t)', right: 'Create context vector from all states' },
        { id: 'p4', left: 'Alignment model (a)', right: 'Neural network that computes compatibility' },
      ],
      explanation: 'Attention works in three steps: (1) Score each encoder state for relevance to the current decoder state, (2) Normalize scores via softmax to get attention weights that sum to 1, (3) Compute a weighted sum of encoder states as the context vector. The alignment model is the function that computes the scores.',
      points: 20,
    },
    {
      id: 'q3-soft-vs-hard',
      type: 'multiple_choice',
      question: 'Why is soft attention preferred over hard attention in practice?',
      options: [
        'Soft attention is faster',
        'Soft attention is differentiable, enabling standard backpropagation',
        'Soft attention uses less memory',
        'Hard attention cannot handle long sequences',
      ],
      correctAnswer: 1,
      explanation: 'Soft attention uses a weighted average (differentiable), allowing standard backpropagation training. Hard attention makes discrete selections (non-differentiable), requiring reinforcement learning techniques like REINFORCE, which have high variance. The ease of training makes soft attention dominant in practice.',
      points: 10,
    },
    {
      id: 'q4-context-vector',
      type: 'multiple_choice',
      question: 'How does the attention context vector differ from the encoder-decoder bottleneck vector?',
      options: [
        'The context vector is larger in dimension',
        'The context vector is computed once; the bottleneck is computed for each output',
        'The context vector is different for each decoder step; the bottleneck is fixed',
        'There is no difference; they are the same thing',
      ],
      correctAnswer: 2,
      explanation: 'The key difference is that the attention context vector c_t is computed freshly for each decoder timestep t, using different attention weights. The encoder-decoder bottleneck is a single fixed vector used for all decoder steps. This dynamic context is what solves the bottleneck problem.',
      points: 10,
    },
    {
      id: 'q5-ntm-memory',
      type: 'multiple_choice',
      question: 'What do Neural Turing Machines (NTMs) use attention for?',
      options: [
        'Only for translation tasks',
        'Reading from and writing to external memory',
        'Compressing the input sequence',
        'Generating images',
      ],
      correctAnswer: 1,
      explanation: 'Neural Turing Machines use attention as a mechanism for addressing external memory. The network learns to read from and write to memory locations using attention-based (differentiable) addressing. This allows NTMs to learn algorithms like copying and sorting by manipulating memory.',
      points: 10,
    },
    {
      id: 'q6-self-attention',
      type: 'multiple_choice',
      question: 'What is self-attention, the key innovation in Transformers?',
      options: [
        'Attention that only looks at the current position',
        'A sequence attending to itself, where each position attends to all others',
        'Attention without any learned parameters',
        'Attention that only works on text data',
      ],
      correctAnswer: 1,
      explanation: 'In self-attention, a sequence attends to itself - each position computes attention weights over all positions in the same sequence. This enables parallel computation (no sequential dependency) and direct connections between any two positions, solving the long-range dependency problem that plagued RNNs.',
      points: 10,
    },
    {
      id: 'q7-transformer-advantage',
      type: 'multiple_choice',
      question: 'What is a key advantage of Transformers (self-attention only) over RNN+Attention?',
      options: [
        'Transformers are smaller in size',
        'Transformers can be trained in parallel across sequence positions',
        'Transformers require less training data',
        'Transformers only work for short sequences',
      ],
      correctAnswer: 1,
      explanation: 'RNNs must process sequences step-by-step (h_t depends on h_{t-1}). Transformers with self-attention can process all positions in parallel since each position directly attends to all others without sequential dependency. This enables massive parallelization on GPUs, making training much faster.',
      points: 10,
    },
    {
      id: 'q8-attention-visualization',
      type: 'multiple_choice',
      question: 'In machine translation with attention, what would you expect to see in the attention weights when generating the word "black" from "Le chat noir dort"?',
      options: [
        'Equal weights on all French words',
        'High weight on "noir" (the French word for black)',
        'High weight on "Le" (the first word)',
        'High weight on "dort" (the verb)',
      ],
      correctAnswer: 1,
      explanation: 'When generating "black", the model should attend strongly to "noir" (the French word for black). This is the power of attention - it learns alignments between source and target words. The attention weights effectively show which input words are relevant for producing each output word.',
      points: 10,
    },
  ],
  shuffleQuestions: false,
  shuffleOptions: false,
  showFeedback: 'immediate',
  allowRetry: true,
  passingScore: 70,
}
